# -*- coding: utf-8 -*-
"""Diamond Price Prediction Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1I0HjlB00bqHlccAT_ZOTxLj1PKqduiFc

# Getting started with Machine Learning

## Steps involved :
##### 1. Import the packages
##### 2. Reading the CSV file
##### 3. Cleaning the data - Numpy/Pandas
##### 4. Visualize the clean data - matplotlib...
##### 5. Machine learning
"""

# import
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn  as sns

# read  dataset
#url="https://raw.githubusercontent.com/sarwansingh/Python/master/ClassExamples/data/diamond.csv"
url = "https://raw.githubusercontent.com/sarwansingh/Python/master/ClassExamples/data/diamond.csv"
df=pd.read_csv(url)

df.head()

#drop 1st column
df.drop('Unnamed: 0', axis=1, inplace=True)

df.head()

df.cut.replace(['Premium', 'Ideal','Good','Very Good','Fair'], [1, 2,3,4,5], inplace=True)
df.color.replace(['J', 'I', 'H', 'G', 'F', 'E', 'D'], [1,2,3,4,5,6,7], inplace=True)
df.clarity.replace(['I1', 'SI2', 'SI1', 'VS2', 'VS1', 'VVS2', 'VVS1', 'IF'], [1,2,3,4,5,6,7,8], inplace=True)

df

df.info()

df.x.isnull()

df.describe()

#Min value of "x", "y", "z" are zero this indicates that there are faulty values in data that represents dimensionless or 2-dimensional diamonds. So we need to filter out those as it clearly faulty data points.
#Dropping dimentionless diamonds

#df = df[(df.x > 0) & (df.y > 0) & (df.z > 0)]

df= df.drop(df[(df.x == 0) & (df.y == 0) & (df.z == 0)].index)

df.shape

df.describe()

sns.pairplot(df)
plt.show()

#There are some features with datapoint that are far from the rest of the dataset which will affect the outcome of our regression model.
#"y" and "z" have some dimensional outliers in our dataset that needs to be eliminated.
#The "depth" should be capped(to set the boundaries so that its value doesn't exeed) but we must examine the regression line to be sure.
#The "table" featured should be capped too.

#Let's have a look at regression plots to get a close look at the outliers.
#lmplot()-linear model plot :- It creates a scatter plot with a linear fit on top of it.


sns.set_style('whitegrid')
sns.lmplot(x ='price', y ='x', data = df)
plt.show()

sns.set_style('whitegrid')
sns.lmplot(x ='price', y ='y', data = df)

sns.set_style('whitegrid')
sns.lmplot(x ='price', y ='z', data = df)

sns.set_style('darkgrid')
sns.lmplot(x ='price', y ='depth', data = df)

sns.set_style('ticks')
sns.lmplot(x ='table', y ='price', data = df)

#Dropping the outliers.
df = df[(df["depth"]<75)&(df["depth"]>45)]
df = df[(df["table"]<80)&(df["table"]>40)]
df = df[(df["x"]<30)]
df = df[(df["y"]<30)]
df = df[(df["z"]<30)&(df["z"]>2)]
df.shape

df

"""# Correlation"""

cmap = sns.diverging_palette(70,20,s=50, l=40, n=6,as_cmap=True)
corrmat= df.corr()
f, ax = plt.subplots(figsize=(12,12))
sns.heatmap(corrmat,cmap=cmap,annot=True )

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.pipeline import Pipeline
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.linear_model import LinearRegression
from xgboost import XGBRegressor
from sklearn.neighbors import KNeighborsRegressor
from sklearn.model_selection import cross_val_score
from sklearn.metrics import mean_squared_error
from sklearn import metrics

X=df.drop('price',axis=1)
y=df['price']
X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.25, random_state=7)

# Building pipelins of standard scaler and model for varios regressors.

pipeline_lr=Pipeline([("scalar1",StandardScaler()),
                     ("lr_classifier",LinearRegression())])

pipeline_dt=Pipeline([("scalar2",StandardScaler()),
                     ("dt_classifier",DecisionTreeRegressor())])

pipeline_rf=Pipeline([("scalar3",StandardScaler()),
                     ("rf_classifier",RandomForestRegressor())])


pipeline_kn=Pipeline([("scalar4",StandardScaler()),
                     ("rf_classifier",KNeighborsRegressor())])


pipeline_xgb=Pipeline([("scalar5",StandardScaler()),
                     ("rf_classifier",XGBRegressor())])

# List of all the pipelines
pipelines = [pipeline_lr, pipeline_dt, pipeline_rf, pipeline_kn, pipeline_xgb]

# Dictionary of pipelines and model types for ease of reference
pipe_dict = {0: "LinearRegression", 1: "DecisionTree", 2: "RandomForest",3: "KNeighbors", 4: "XGBRegressor"}

# Fit the pipelines
for pipe in pipelines:
    pipe.fit(X_train, y_train)

#Testing the Model with the best score on the test set
cv_results_rms = []
for i, model in enumerate(pipelines):
    cv_score = cross_val_score(model, X_train,y_train,scoring="neg_root_mean_squared_error", cv=10)
    cv_results_rms.append(cv_score)
    print("%s: %f " % (pipe_dict[i], cv_score.mean()))

##In the above scores, RandomForest appears to be the model with the best scoring on negative root mean square error

pred = pipeline_rf.predict(X_test)
pred

print("R^2:",metrics.r2_score(y_test, pred))
print("Adjusted R^2:",1 - (1-metrics.r2_score(y_test, pred))*(len(y_test)-1)/(len(y_test)-X_test.shape[1]-1))
print("MAE:",metrics.mean_absolute_error(y_test, pred))
print("MSE:",metrics.mean_squared_error(y_test, pred))
print("RMSE:",np.sqrt(metrics.mean_squared_error(y_test, pred)))

#R^2 (R-squared) is a statistical measure that represents the proportion of the variance for a dependent variable that's explained by an independent variable or variables in a regression model.
#Adjusted R^2 (Adjusted R-squared) is a modified version of R-squared that adjusts for the number of predictors in a regression model.
#MAE(Mean Absolute Error ) measures the average magnitude of the errors in a set of predictions, without considering their direction.
#MSE(Mean Squared Error) measures the average squared difference between actual and predicted values.
#RMSE(Root Mean Squared Error) is the square root of the MSE, representing the standard deviation of the residuals (prediction errors).

pipeline_rf.predict([[0.32, 3, 3, 2, 63.1, 56.0, 4.34, 4.37,2.75]])

pred = pipeline_rf.predict([[0.32, 3, 3, 2, 63.1, 56.0, 4.34, 4.37,2.75 ]])
op_1=str(round(pred[0])) + ' la'
op_1

#efficiency
pipeline_rf.score(X_train, y_train)

"""# Altogether"""

import pandas as pd
import numpy as np
from sklearn.linear_model import LinearRegression
#from sklearn.pipeline import Pipeline
#from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestRegressor
url="https://raw.githubusercontent.com/sarwansingh/Python/master/ClassExamples/data/diamond.csv"
df=pd.read_csv(url)
df.drop('Unnamed: 0', axis=1, inplace=True)
df.cut.replace(['Premium', 'Ideal','Good','Very Good','Fair'], [1, 2,3,4,5], inplace=True)
df.color.replace(['D','E','F','G','H','I','J'], [1,2,3,4,5,6,7], inplace=True)
df.clarity.replace(['I1', 'SI2', 'SI1', 'VS2', 'VS1', 'VVS2', 'VVS1', 'IF'], [1,2,3,4,5,6,7,8], inplace=True)

df = df.drop(df[df["x"]==0].index)
df = df.drop(df[df["y"]==0].index)
df= df.drop(df[df["z"]==0].index)

#Dropping the outliers.
df = df[(df["depth"]<75)&(df["depth"]>45)]
df = df[(df["table"]<80)&(df["table"]>40)]
df = df[(df["x"]<30)]
df = df[(df["y"]<30)]
df = df[(df["z"]<30)&(df["z"]>2)]

X= df.drop(["price"],axis =1)
Y= df["price"]

#pipeline_rf=Pipeline([("scalar3",StandardScaler()),"rf_classifier",RandomForestRegressor())])
#pipeline_rf.fit(X,Y)
model_1=RandomForestRegressor()
model_1.fit(X,Y)

#0.32, 3, 3, 2, 63.1, 56.0, 4.34, 4.37,2.75
pred=model_1.predict([[0.32, 3, 3, 2, 63.1, 56.0, 4.34, 4.37,2.75]])
op_1=str(round(pred[0])) + ' la'
op_1

